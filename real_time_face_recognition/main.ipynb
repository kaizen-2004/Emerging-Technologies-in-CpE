{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b05b2333-fd30-4ee9-949b-2a5fcecccf12",
   "metadata": {},
   "source": [
    "### Create a Python Virtual Environment and Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd1e08e-bc1b-4b6a-81d8-4b8ae8d158be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv ~/venvs/face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "179c7c45-77fd-4063-a0e1-9ea5ebcbc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ~/venvs/face_recognition/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a8aed-ca4d-4471-b400-7a61d02943c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058beab-dc9d-42b9-bbbc-72f617f6b690",
   "metadata": {},
   "source": [
    "### Folder Structure Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f11899-2d43-416a-89bf-5a6b367d0245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter person name:  Laurentti\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit anytime.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "person_name = input(\"Enter person name: \")\n",
    "save_dir = os.path.join(\"dataset-raw\", person_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "max_images = 50       # Number of images to capture\n",
    "frame_interval = 10    # Capture every N frames\n",
    "conf_threshold = 0.5  # Minimum confidence for face detection\n",
    "# ================================================\n",
    "\n",
    "# Load OpenCV DNN face detector\n",
    "model_file = \"res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "config_file = \"deploy.prototxt\"\n",
    "if not os.path.exists(model_file) or not os.path.exists(config_file):\n",
    "    raise FileNotFoundError(\"Make sure deploy.prototxt and caffemodel are in the working directory.\")\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(config_file, model_file)\n",
    "\n",
    "def detect_faces(frame, conf_threshold=0.5):\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    faces = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(w, x2), min(h, y2)\n",
    "            faces.append((x1, y1, x2-x1, y2-y1))\n",
    "    return faces\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "frame_count = 0\n",
    "\n",
    "print(\"Press 'q' to quit anytime.\")\n",
    "\n",
    "while count < max_images:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    faces = detect_faces(frame, conf_threshold)\n",
    "    for (x, y, w, h) in faces:\n",
    "        if frame_count % frame_interval == 0:\n",
    "            face_crop = frame[y:y+h, x:x+w]\n",
    "            filename = os.path.join(save_dir, f\"{count+1}.jpg\")\n",
    "            cv2.imwrite(filename, face_crop)\n",
    "            count += 1\n",
    "            print(f\"Saved: {filename}\")\n",
    "        # Draw rectangle for visualization\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Capture Dataset\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"‚úÖ Capture complete! {count} images saved for {person_name}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc47a4-a008-42a1-b60f-e10c3590fb24",
   "metadata": {},
   "source": [
    "### Auto Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e251b4-3c31-427e-acc7-318fe6c059ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ OpenCV DNN Face Annotation Pipeline\n",
      "============================================================\n",
      "‚úÖ Model files found, loading face detector...\n",
      "‚úÖ Face detector loaded successfully!\n",
      "üöÄ Starting automatic face annotation with OpenCV DNN...\n",
      "============================================================\n",
      "\n",
      "üîç Processing: Laurentti\n",
      "  ‚úÖ 1.jpg: 1 face(s) detected\n",
      "  ‚úÖ 10.jpg: 1 face(s) detected\n",
      "  ‚úÖ 11.jpg: 1 face(s) detected\n",
      "  ‚úÖ 12.jpg: 1 face(s) detected\n",
      "  ‚úÖ 13.jpg: 1 face(s) detected\n",
      "  ‚úÖ 14.jpg: 1 face(s) detected\n",
      "  ‚úÖ 15.jpg: 1 face(s) detected\n",
      "  ‚úÖ 16.jpg: 1 face(s) detected\n",
      "  ‚úÖ 17.jpg: 1 face(s) detected\n",
      "  ‚úÖ 18.jpg: 1 face(s) detected\n",
      "  ‚úÖ 19.jpg: 1 face(s) detected\n",
      "  ‚úÖ 2.jpg: 1 face(s) detected\n",
      "  ‚úÖ 20.jpg: 1 face(s) detected\n",
      "  ‚úÖ 21.jpg: 1 face(s) detected\n",
      "  ‚úÖ 22.jpg: 1 face(s) detected\n",
      "  ‚úÖ 23.jpg: 1 face(s) detected\n",
      "  ‚úÖ 24.jpg: 1 face(s) detected\n",
      "  ‚úÖ 25.jpg: 1 face(s) detected\n",
      "  ‚úÖ 26.jpg: 1 face(s) detected\n",
      "  ‚úÖ 27.jpg: 1 face(s) detected\n",
      "  ‚úÖ 28.jpg: 1 face(s) detected\n",
      "  ‚úÖ 29.jpg: 1 face(s) detected\n",
      "  ‚úÖ 3.jpg: 1 face(s) detected\n",
      "  ‚úÖ 30.jpg: 1 face(s) detected\n",
      "  ‚úÖ 31.jpg: 1 face(s) detected\n",
      "  ‚úÖ 32.jpg: 1 face(s) detected\n",
      "  ‚úÖ 33.jpg: 1 face(s) detected\n",
      "  ‚úÖ 34.jpg: 1 face(s) detected\n",
      "  ‚úÖ 35.jpg: 1 face(s) detected\n",
      "  ‚úÖ 36.jpg: 1 face(s) detected\n",
      "  ‚úÖ 37.jpg: 1 face(s) detected\n",
      "  ‚úÖ 38.jpg: 1 face(s) detected\n",
      "  ‚úÖ 39.jpg: 1 face(s) detected\n",
      "  ‚úÖ 4.jpg: 1 face(s) detected\n",
      "  ‚úÖ 40.jpg: 1 face(s) detected\n",
      "  ‚úÖ 41.jpg: 1 face(s) detected\n",
      "  ‚úÖ 42.jpg: 1 face(s) detected\n",
      "  ‚úÖ 43.jpg: 1 face(s) detected\n",
      "  ‚úÖ 44.jpg: 1 face(s) detected\n",
      "  ‚úÖ 45.jpg: 1 face(s) detected\n",
      "  ‚úÖ 46.jpg: 1 face(s) detected\n",
      "  ‚úÖ 47.jpg: 1 face(s) detected\n",
      "  ‚úÖ 48.jpg: 1 face(s) detected\n",
      "  ‚úÖ 49.jpg: 1 face(s) detected\n",
      "  ‚úÖ 5.jpg: 1 face(s) detected\n",
      "  ‚úÖ 50.jpg: 1 face(s) detected\n",
      "  ‚úÖ 6.jpg: 1 face(s) detected\n",
      "  ‚úÖ 7.jpg: 1 face(s) detected\n",
      "  ‚úÖ 8.jpg: 1 face(s) detected\n",
      "  ‚úÖ 9.jpg: 1 face(s) detected\n",
      "  üìä Laurentti: 50 faces in 50 images\n",
      "\n",
      "üîç Processing: Steve\n",
      "  ‚úÖ 1.jpg: 1 face(s) detected\n",
      "  ‚úÖ 10.jpg: 1 face(s) detected\n",
      "  ‚úÖ 11.jpg: 1 face(s) detected\n",
      "  ‚úÖ 12.jpg: 1 face(s) detected\n",
      "  ‚úÖ 13.jpg: 1 face(s) detected\n",
      "  ‚úÖ 14.jpg: 1 face(s) detected\n",
      "  ‚úÖ 15.jpg: 1 face(s) detected\n",
      "  ‚úÖ 16.jpg: 1 face(s) detected\n",
      "  ‚úÖ 17.jpg: 1 face(s) detected\n",
      "  ‚úÖ 18.jpg: 1 face(s) detected\n",
      "  ‚úÖ 19.jpg: 1 face(s) detected\n",
      "  ‚úÖ 2.jpg: 1 face(s) detected\n",
      "  ‚úÖ 20.jpg: 1 face(s) detected\n",
      "  ‚úÖ 21.jpg: 1 face(s) detected\n",
      "  ‚úÖ 22.jpg: 1 face(s) detected\n",
      "  ‚úÖ 23.jpg: 1 face(s) detected\n",
      "  ‚úÖ 24.jpg: 1 face(s) detected\n",
      "  ‚úÖ 25.jpg: 1 face(s) detected\n",
      "  ‚úÖ 26.jpg: 1 face(s) detected\n",
      "  ‚úÖ 27.jpg: 1 face(s) detected\n",
      "  ‚úÖ 28.jpg: 1 face(s) detected\n",
      "  ‚úÖ 29.jpg: 1 face(s) detected\n",
      "  ‚úÖ 3.jpg: 1 face(s) detected\n",
      "  ‚úÖ 30.jpg: 1 face(s) detected\n",
      "  ‚úÖ 31.jpg: 1 face(s) detected\n",
      "  ‚úÖ 32.jpg: 1 face(s) detected\n",
      "  ‚úÖ 33.jpg: 1 face(s) detected\n",
      "  ‚úÖ 34.jpg: 1 face(s) detected\n",
      "  ‚úÖ 35.jpg: 1 face(s) detected\n",
      "  ‚úÖ 36.jpg: 1 face(s) detected\n",
      "  ‚úÖ 37.jpg: 1 face(s) detected\n",
      "  ‚úÖ 38.jpg: 1 face(s) detected\n",
      "  ‚úÖ 39.jpg: 1 face(s) detected\n",
      "  ‚úÖ 4.jpg: 1 face(s) detected\n",
      "  ‚úÖ 40.jpg: 1 face(s) detected\n",
      "  ‚úÖ 41.jpg: 1 face(s) detected\n",
      "  ‚úÖ 42.jpg: 1 face(s) detected\n",
      "  ‚úÖ 43.jpg: 1 face(s) detected\n",
      "  ‚úÖ 44.jpg: 1 face(s) detected\n",
      "  ‚úÖ 45.jpg: 1 face(s) detected\n",
      "  ‚úÖ 46.jpg: 1 face(s) detected\n",
      "  ‚úÖ 47.jpg: 1 face(s) detected\n",
      "  ‚úÖ 48.jpg: 1 face(s) detected\n",
      "  ‚úÖ 49.jpg: 1 face(s) detected\n",
      "  ‚úÖ 5.jpg: 1 face(s) detected\n",
      "  ‚úÖ 50.jpg: 1 face(s) detected\n",
      "  ‚úÖ 6.jpg: 1 face(s) detected\n",
      "  ‚úÖ 7.jpg: 1 face(s) detected\n",
      "  ‚úÖ 8.jpg: 1 face(s) detected\n",
      "  ‚úÖ 9.jpg: 1 face(s) detected\n",
      "  üìä Steve: 50 faces in 50 images\n",
      "\n",
      "============================================================\n",
      "üìä ANNOTATION STATISTICS\n",
      "============================================================\n",
      "Total images processed: 100\n",
      "Images with faces: 100\n",
      "Images without faces: 0\n",
      "Total faces detected: 100\n",
      "Success rate: 100.0%\n",
      "============================================================\n",
      "üìÑ JSON annotations saved to: /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/annotations/face_annotations.json\n",
      "üìÑ XML annotations saved to: /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/annotations/face_annotations.xml\n",
      "üìÑ Individual XML annotations saved to: /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/annotations/annotations\n",
      "\n",
      "‚úÖ Annotation completed successfully!\n",
      "üìÅ Output files:\n",
      "   - /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/annotations/face_annotations.json\n",
      "   - /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/annotations/face_annotations.xml\n",
      "   - /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/annotations/annotations\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "base_dir = os.getcwd() \n",
    "annotations_folder = os.path.join(base_dir, \"annotations\")\n",
    "\n",
    "if not os.path.exists(annotations_folder):\n",
    "    os.makedirs(annotations_folder)\n",
    "\n",
    "class OpenCVDNNFaceDetector:\n",
    "    def __init__(self, confidence_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Initialize OpenCV DNN Face Detector with pre-trained model\n",
    "        \"\"\"\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        # Model files\n",
    "        self.model_file = \"res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "        self.config_file = \"deploy.prototxt\"\n",
    "        \n",
    "        # Verify files exist\n",
    "        if not os.path.exists(self.model_file):\n",
    "            available_models = [f for f in os.listdir('.') if f.endswith('.caffemodel')]\n",
    "            raise FileNotFoundError(f\"Model file not found. Available: {available_models}\")\n",
    "        if not os.path.exists(self.config_file):\n",
    "            available_configs = [f for f in os.listdir('.') if 'deploy' in f and '.prototxt' in f]\n",
    "            raise FileNotFoundError(f\"Config file not found. Available: {available_configs}\")\n",
    "        \n",
    "        print(\"‚úÖ Model files found, loading face detector...\")\n",
    "        \n",
    "        # Load the model\n",
    "        self.net = cv2.dnn.readNetFromCaffe(self.config_file, self.model_file)\n",
    "        print(\"‚úÖ Face detector loaded successfully!\")\n",
    "    \n",
    "    def detect_faces(self, image_path):\n",
    "        \"\"\"Detect faces using OpenCV DNN\"\"\"\n",
    "        # Read image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"‚ö†Ô∏è  Could not read image: {image_path}\")\n",
    "            return [], (0, 0)\n",
    "        \n",
    "        (h, w) = image.shape[:2]\n",
    "        \n",
    "        # Create blob from image\n",
    "        blob = cv2.dnn.blobFromImage(\n",
    "            cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "            (104.0, 177.0, 123.0)\n",
    "        )\n",
    "        \n",
    "        # Pass blob through network\n",
    "        self.net.setInput(blob)\n",
    "        detections = self.net.forward()\n",
    "        \n",
    "        faces = []\n",
    "        \n",
    "        for i in range(0, detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            \n",
    "            if confidence > self.confidence_threshold:\n",
    "                # Extract bounding box coordinates\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "                \n",
    "                # Ensure coordinates are within image boundaries\n",
    "                x1 = max(0, x1)\n",
    "                y1 = max(0, y1)\n",
    "                x2 = min(w, x2)\n",
    "                y2 = min(h, y2)\n",
    "                \n",
    "                width = x2 - x1\n",
    "                height = y2 - y1\n",
    "                \n",
    "                # Only include valid detections\n",
    "                if width > 0 and height > 0:\n",
    "                    faces.append({\n",
    "                        'bbox': (x1, y1, width, height),\n",
    "                        'confidence': confidence\n",
    "                    })\n",
    "        \n",
    "        return faces, image.shape\n",
    "\n",
    "def annotate_dataset_opencv_dnn(dataset_folder, output_format='both', min_confidence=0.5):\n",
    "    \"\"\"\n",
    "    Automatically annotate faces using OpenCV DNN\n",
    "    \"\"\"\n",
    "    detector = OpenCVDNNFaceDetector(confidence_threshold=min_confidence)\n",
    "    annotations = {}\n",
    "    stats = {\n",
    "        'total_images': 0,\n",
    "        'images_with_faces': 0,\n",
    "        'total_faces': 0,\n",
    "        'failed_detections': 0\n",
    "    }\n",
    "    \n",
    "    print(\"üöÄ Starting automatic face annotation with OpenCV DNN...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for person_folder in sorted(os.listdir(dataset_folder)):\n",
    "        person_path = os.path.join(dataset_folder, person_folder)\n",
    "        \n",
    "        if not os.path.isdir(person_path):\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüîç Processing: {person_folder}\")\n",
    "        person_images = 0\n",
    "        person_faces = 0\n",
    "        \n",
    "        for image_file in sorted(os.listdir(person_path)):\n",
    "            if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_path = os.path.join(person_path, image_file)\n",
    "                stats['total_images'] += 1\n",
    "                person_images += 1\n",
    "                \n",
    "                # Detect faces with OpenCV DNN\n",
    "                faces, image_shape = detector.detect_faces(image_path)\n",
    "                \n",
    "                # Filter by confidence\n",
    "                confident_faces = [f for f in faces if f['confidence'] >= min_confidence]\n",
    "                \n",
    "                if len(confident_faces) > 0:\n",
    "                    stats['images_with_faces'] += 1\n",
    "                    person_faces += len(confident_faces)\n",
    "                    stats['total_faces'] += len(confident_faces)\n",
    "                    \n",
    "                    # Store annotation\n",
    "                    image_key = f\"{person_folder}/{image_file}\"\n",
    "                    annotations[image_key] = {\n",
    "                        'person': person_folder,\n",
    "                        'image_path': image_path,\n",
    "                        'image_size': {\n",
    "                            'width': image_shape[1],\n",
    "                            'height': image_shape[0],\n",
    "                            'channels': image_shape[2] if len(image_shape) > 2 else 3\n",
    "                        },\n",
    "                        'faces': [],\n",
    "                        'face_count': len(confident_faces),\n",
    "                        'detection_confidence': min_confidence\n",
    "                    }\n",
    "                    \n",
    "                    for i, face in enumerate(confident_faces):\n",
    "                        x, y, w, h = face['bbox']\n",
    "                        annotations[image_key]['faces'].append({\n",
    "                            'face_id': i,\n",
    "                            'bbox': {\n",
    "                                'x': int(x),\n",
    "                                'y': int(y),\n",
    "                                'width': int(w),\n",
    "                                'height': int(h)\n",
    "                            },\n",
    "                            'confidence': float(face['confidence']),\n",
    "                            'bbox_normalized': {\n",
    "                                'x': x / image_shape[1],\n",
    "                                'y': y / image_shape[0],\n",
    "                                'width': w / image_shape[1],\n",
    "                                'height': h / image_shape[0]\n",
    "                            }\n",
    "                        })\n",
    "                    \n",
    "                    print(f\"  ‚úÖ {image_file}: {len(confident_faces)} face(s) detected\")\n",
    "                else:\n",
    "                    stats['failed_detections'] += 1\n",
    "                    print(f\"  ‚ö†Ô∏è  {image_file}: No faces detected\")\n",
    "        \n",
    "        print(f\"  üìä {person_folder}: {person_faces} faces in {person_images} images\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print_stats(stats)\n",
    "    \n",
    "    # Export annotations\n",
    "    output_files = []\n",
    "    if output_format in ['json', 'both']:\n",
    "        json_file = export_json_annotations(annotations, annotations_folder, stats)\n",
    "        output_files.append(json_file)\n",
    "    \n",
    "    if output_format in ['xml', 'both']:\n",
    "        xml_file = export_xml_annotations(annotations, annotations_folder, stats)\n",
    "        output_files.append(xml_file)\n",
    "    \n",
    "    # Export individual XML files (PASCAL VOC format)\n",
    "    if output_format in ['xml', 'both']:\n",
    "        voc_folder = export_individual_xml_annotations(annotations, annotations_folder)\n",
    "        output_files.append(voc_folder)\n",
    "    \n",
    "    return annotations, output_files\n",
    "\n",
    "def print_stats(stats):\n",
    "    \"\"\"Print annotation statistics\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä ANNOTATION STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total images processed: {stats['total_images']}\")\n",
    "    print(f\"Images with faces: {stats['images_with_faces']}\")\n",
    "    print(f\"Images without faces: {stats['failed_detections']}\")\n",
    "    print(f\"Total faces detected: {stats['total_faces']}\")\n",
    "    \n",
    "    if stats['total_images'] > 0:\n",
    "        success_rate = (stats['images_with_faces'] / stats['total_images']) * 100\n",
    "        print(f\"Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def export_json_annotations(annotations, output_folder, stats):\n",
    "    \"\"\"Export annotations to JSON format\"\"\"\n",
    "    output_data = {\n",
    "        'metadata': {\n",
    "            'dataset_name': 'Face Recognition Dataset',\n",
    "            'annotation_tool': 'OpenCV DNN Face Detection',\n",
    "            'total_images': stats['total_images'],\n",
    "            'images_with_faces': stats['images_with_faces'],\n",
    "            'total_faces': stats['total_faces'],\n",
    "            'export_timestamp': str(np.datetime64('now'))\n",
    "        },\n",
    "        'annotations': annotations\n",
    "    }\n",
    "    \n",
    "    output_path = os.path.join(annotations_folder, 'face_annotations.json')\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"üìÑ JSON annotations saved to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def export_xml_annotations(annotations, output_folder, stats):\n",
    "    \"\"\"Export annotations to single XML file\"\"\"\n",
    "    root = ET.Element(\"face_annotations\")\n",
    "    \n",
    "    # Add metadata\n",
    "    metadata = ET.SubElement(root, \"metadata\")\n",
    "    ET.SubElement(metadata, \"dataset_name\").text = \"Face Recognition Dataset\"\n",
    "    ET.SubElement(metadata, \"annotation_tool\").text = \"OpenCV DNN Face Detection\"\n",
    "    ET.SubElement(metadata, \"total_images\").text = str(stats['total_images'])\n",
    "    ET.SubElement(metadata, \"images_with_faces\").text = str(stats['images_with_faces'])\n",
    "    ET.SubElement(metadata, \"total_faces\").text = str(stats['total_faces'])\n",
    "    \n",
    "    # Add annotations\n",
    "    annotations_elem = ET.SubElement(root, \"images\")\n",
    "    for image_key, annotation in annotations.items():\n",
    "        image_elem = ET.SubElement(annotations_elem, \"image\")\n",
    "        \n",
    "        ET.SubElement(image_elem, \"person\").text = annotation['person']\n",
    "        ET.SubElement(image_elem, \"file_path\").text = annotation['image_path']\n",
    "        \n",
    "        size_elem = ET.SubElement(image_elem, \"size\")\n",
    "        ET.SubElement(size_elem, \"width\").text = str(annotation['image_size']['width'])\n",
    "        ET.SubElement(size_elem, \"height\").text = str(annotation['image_size']['height'])\n",
    "        \n",
    "        faces_elem = ET.SubElement(image_elem, \"faces\")\n",
    "        for face in annotation['faces']:\n",
    "            face_elem = ET.SubElement(faces_elem, \"face\")\n",
    "            ET.SubElement(face_elem, \"id\").text = str(face['face_id'])\n",
    "            ET.SubElement(face_elem, \"confidence\").text = str(face['confidence'])\n",
    "            \n",
    "            bbox_elem = ET.SubElement(face_elem, \"bndbox\")\n",
    "            ET.SubElement(bbox_elem, \"xmin\").text = str(face['bbox']['x'])\n",
    "            ET.SubElement(bbox_elem, \"ymin\").text = str(face['bbox']['y'])\n",
    "            ET.SubElement(bbox_elem, \"xmax\").text = str(face['bbox']['x'] + face['bbox']['width'])\n",
    "            ET.SubElement(bbox_elem, \"ymax\").text = str(face['bbox']['y'] + face['bbox']['height'])\n",
    "    \n",
    "    # Pretty print XML\n",
    "    xml_str = minidom.parseString(ET.tostring(root)).toprettyxml(indent=\"  \")\n",
    "    \n",
    "    output_path = os.path.join(annotations_folder, 'face_annotations.xml')\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(xml_str)\n",
    "    \n",
    "    print(f\"üìÑ XML annotations saved to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def export_individual_xml_annotations(annotations, output_folder):\n",
    "    \"\"\"Export individual XML files for each image (PASCAL VOC format)\"\"\"\n",
    "    annotations_folder = os.path.join(output_folder, \"annotations\")\n",
    "    if not os.path.exists(annotations_folder):\n",
    "        os.makedirs(annotations_folder)\n",
    "    \n",
    "    for image_key, annotation in annotations.items():\n",
    "        root = ET.Element(\"annotation\")\n",
    "        \n",
    "        # Folder and filename\n",
    "        ET.SubElement(root, \"folder\").text = \"dataset\"\n",
    "        ET.SubElement(root, \"filename\").text = os.path.basename(annotation['image_path'])\n",
    "        ET.SubElement(root, \"path\").text = annotation['image_path']\n",
    "        \n",
    "        # Source\n",
    "        source = ET.SubElement(root, \"source\")\n",
    "        ET.SubElement(source, \"database\").text = \"Custom Face Dataset\"\n",
    "        ET.SubElement(source, \"annotation\").text = \"OpenCV DNN Face Detection\"\n",
    "        \n",
    "        # Image size\n",
    "        size = ET.SubElement(root, \"size\")\n",
    "        ET.SubElement(size, \"width\").text = str(annotation['image_size']['width'])\n",
    "        ET.SubElement(size, \"height\").text = str(annotation['image_size']['height'])\n",
    "        ET.SubElement(size, \"depth\").text = \"3\"\n",
    "        \n",
    "        # Segmentation (not used, but required by some tools)\n",
    "        ET.SubElement(root, \"segmented\").text = \"0\"\n",
    "        \n",
    "        # Each face as an object\n",
    "        for face in annotation['faces']:\n",
    "            obj = ET.SubElement(root, \"object\")\n",
    "            ET.SubElement(obj, \"name\").text = annotation['person']\n",
    "            ET.SubElement(obj, \"pose\").text = \"Unspecified\"\n",
    "            ET.SubElement(obj, \"truncated\").text = \"0\"\n",
    "            ET.SubElement(obj, \"difficult\").text = \"0\"\n",
    "            ET.SubElement(obj, \"confidence\").text = str(face['confidence'])\n",
    "            \n",
    "            bndbox = ET.SubElement(obj, \"bndbox\")\n",
    "            ET.SubElement(bndbox, \"xmin\").text = str(face['bbox']['x'])\n",
    "            ET.SubElement(bndbox, \"ymin\").text = str(face['bbox']['y'])\n",
    "            ET.SubElement(bndbox, \"xmax\").text = str(face['bbox']['x'] + face['bbox']['width'])\n",
    "            ET.SubElement(bndbox, \"ymax\").text = str(face['bbox']['y'] + face['bbox']['height'])\n",
    "        \n",
    "        # Save individual XML\n",
    "        xml_str = minidom.parseString(ET.tostring(root)).toprettyxml(indent=\"  \")\n",
    "        xml_filename = os.path.splitext(os.path.basename(annotation['image_path']))[0] + \".xml\"\n",
    "        xml_path = os.path.join(annotations_folder, xml_filename)\n",
    "        \n",
    "        with open(xml_path, 'w') as f:\n",
    "            f.write(xml_str)\n",
    "    \n",
    "    print(f\"üìÑ Individual XML annotations saved to: {annotations_folder}\")\n",
    "    return annotations_folder\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the complete annotation pipeline\"\"\"\n",
    "    dataset_folder = \"dataset-raw\"  # Your standardized dataset\n",
    "    output_format = \"both\"  # 'json', 'xml', or 'both'\n",
    "    min_confidence = 0.5  # Minimum confidence for face detection\n",
    "    \n",
    "    print(\"üéØ OpenCV DNN Face Annotation Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Run annotation\n",
    "        annotations, output_files = annotate_dataset_opencv_dnn(\n",
    "            dataset_folder, \n",
    "            output_format=output_format,\n",
    "            min_confidence=min_confidence\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Annotation completed successfully!\")\n",
    "        print(\"üìÅ Output files:\")\n",
    "        for file_path in output_files:\n",
    "            print(f\"   - {file_path}\")\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå {e}\")\n",
    "        print(\"\\nüí° Please rename your model files to:\")\n",
    "        print(\"   - deploy.prototxt\")\n",
    "        print(\"   - res10_300x300_ssd_iter_140000_fp16.caffemodel\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48f0bb-8043-4e77-92c4-e94df9dd5d35",
   "metadata": {},
   "source": [
    "### Standardize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f0aaf9-bec7-4dad-8c78-ac041f33124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_1.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_10.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_11.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_12.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_13.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_14.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_15.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_16.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_17.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_18.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_19.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_2.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_20.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_21.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_22.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_23.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_24.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_25.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_26.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_27.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_28.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_29.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_3.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_30.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_31.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_32.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_33.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_34.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_35.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_36.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_37.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_38.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_39.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_4.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_40.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_41.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_42.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_43.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_44.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_45.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_46.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_47.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_48.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_49.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_5.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_50.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_6.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_7.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_8.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Laurentti/1_9.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_1.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_10.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_11.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_12.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_13.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_14.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_15.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_16.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_17.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_18.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_19.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_2.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_20.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_21.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_22.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_23.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_24.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_25.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_26.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_27.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_28.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_29.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_3.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_30.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_31.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_32.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_33.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_34.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_35.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_36.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_37.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_38.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_39.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_4.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_40.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_41.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_42.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_43.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_44.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_45.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_46.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_47.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_48.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_49.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_5.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_50.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_6.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_7.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_8.jpg\n",
      "‚úÖ Saved /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/dataset-standardized/Steve/1_9.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "# Paths\n",
    "base_dir = os.getcwd()  # Base project directory\n",
    "annotations_file = os.path.join(base_dir, \"annotations\", \"face_annotations.json\")\n",
    "output_folder = os.path.join(base_dir, \"dataset-standardized\")\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Load annotations\n",
    "with open(annotations_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "annotations = data.get(\"annotations\", {})\n",
    "\n",
    "# Target size for standardization\n",
    "TARGET_SIZE = (224, 224)  # Typical size for InsightFace / ArcFace\n",
    "\n",
    "for image_key, annotation in annotations.items():\n",
    "    person = annotation['person']\n",
    "    person_folder = os.path.join(output_folder, person)\n",
    "    if not os.path.exists(person_folder):\n",
    "        os.makedirs(person_folder)\n",
    "\n",
    "    image_path = annotation['image_path']\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"‚ö†Ô∏è  Could not read {image_path}\")\n",
    "        continue\n",
    "\n",
    "    for i, face in enumerate(annotation['faces']):\n",
    "        x, y, w, h = face['bbox']['x'], face['bbox']['y'], face['bbox']['width'], face['bbox']['height']\n",
    "\n",
    "        # Ensure bounding box is within image\n",
    "        x, y = max(0, x), max(0, y)\n",
    "        w, h = max(1, w), max(1, h)\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "        # Resize to target size\n",
    "        standardized_face = cv2.resize(cropped_face, TARGET_SIZE)\n",
    "\n",
    "        # Save\n",
    "        save_name = f\"{i+1}_{os.path.basename(image_path)}\"\n",
    "        save_path = os.path.join(person_folder, save_name)\n",
    "        cv2.imwrite(save_path, standardized_face)\n",
    "        print(f\"‚úÖ Saved {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ff57d-906a-4099-96d5-a3331abc60ce",
   "metadata": {},
   "source": [
    "### Dataset Augementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7514e98-c90a-4e87-b49c-8cee104f693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Dataset augmentation completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "base_dir = os.getcwd()\n",
    "dataset_folder = os.path.join(base_dir, \"dataset-standardized\")\n",
    "augmented_folder = os.path.join(base_dir, \"dataset-augmented\")\n",
    "\n",
    "if not os.path.exists(augmented_folder):\n",
    "    os.makedirs(augmented_folder)\n",
    "\n",
    "# Parameters\n",
    "ROTATION_ANGLES = [-15, -10, -5, 5, 10, 15]  # degrees\n",
    "BRIGHTNESS_FACTORS = [0.7, 1.0, 1.3]  # multiply pixel values\n",
    "ZOOM_FACTORS = [0.9, 1.0, 1.1]  # crop & resize\n",
    "FLIP_MODES = [0, 1, -1]  # 0=vertical, 1=horizontal, -1=both\n",
    "\n",
    "def augment_image(img):\n",
    "    aug_images = []\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # Rotate\n",
    "    for angle in ROTATION_ANGLES:\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1)\n",
    "        rotated = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "        aug_images.append(rotated)\n",
    "\n",
    "    # Flip\n",
    "    for mode in FLIP_MODES:\n",
    "        flipped = cv2.flip(img, mode)\n",
    "        aug_images.append(flipped)\n",
    "\n",
    "    # Brightness\n",
    "    for factor in BRIGHTNESS_FACTORS:\n",
    "        bright = np.clip(img * factor, 0, 255).astype(np.uint8)\n",
    "        aug_images.append(bright)\n",
    "\n",
    "    # Zoom\n",
    "    for factor in ZOOM_FACTORS:\n",
    "        if factor == 1.0:\n",
    "            aug_images.append(img)\n",
    "            continue\n",
    "        new_h, new_w = int(h*factor), int(w*factor)\n",
    "        y1 = (h - new_h)//2\n",
    "        x1 = (w - new_w)//2\n",
    "        zoomed = img[y1:y1+new_h, x1:x1+new_w]\n",
    "        zoomed = cv2.resize(zoomed, (w, h))\n",
    "        aug_images.append(zoomed)\n",
    "\n",
    "    return aug_images\n",
    "\n",
    "# Iterate over standardized dataset\n",
    "for person in sorted(os.listdir(dataset_folder)):\n",
    "    person_path = os.path.join(dataset_folder, person)\n",
    "    if not os.path.isdir(person_path):\n",
    "        continue\n",
    "\n",
    "    out_person_folder = os.path.join(augmented_folder, person)\n",
    "    if not os.path.exists(out_person_folder):\n",
    "        os.makedirs(out_person_folder)\n",
    "\n",
    "    for img_file in sorted(os.listdir(person_path)):\n",
    "        if not img_file.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            continue\n",
    "        img_path = os.path.join(person_path, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        aug_images = augment_image(img)\n",
    "        for i, aug_img in enumerate(aug_images):\n",
    "            save_name = f\"{os.path.splitext(img_file)[0]}_aug{i+1}.jpg\"\n",
    "            save_path = os.path.join(out_person_folder, save_name)\n",
    "            cv2.imwrite(save_path, aug_img)\n",
    "\n",
    "print(\"üéâ Dataset augmentation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f58204-9a2d-4fd8-9978-1d0c0d4374cf",
   "metadata": {},
   "source": [
    "### Build Face Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f042c4-9083-4909-854b-e68e065cb13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (320, 320)\n",
      "üöÄ Building face embeddings...\n",
      "‚úÖ Laurentti: 50 embeddings\n",
      "‚úÖ Steve: 50 embeddings\n",
      "üéâ Face embeddings saved to: /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/face_database.pkl\n",
      "Total persons: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# Paths\n",
    "base_dir = os.getcwd()\n",
    "dataset_folder = os.path.join(base_dir, \"dataset-standardized\")  # or \"dataset-augmented\"\n",
    "output_file = os.path.join(base_dir, \"face_database.pkl\")\n",
    "\n",
    "# Initialize InsightFace\n",
    "app = FaceAnalysis(providers=['CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(320, 320))  # smaller detection window\n",
    "\n",
    "# Dictionary to store embeddings\n",
    "face_database = {}\n",
    "\n",
    "print(\"üöÄ Building face embeddings...\")\n",
    "\n",
    "for person in sorted(os.listdir(dataset_folder)):\n",
    "    person_path = os.path.join(dataset_folder, person)\n",
    "    if not os.path.isdir(person_path):\n",
    "        continue\n",
    "\n",
    "    embeddings_list = []\n",
    "\n",
    "    for img_file in sorted(os.listdir(person_path)):\n",
    "        if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(person_path, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"‚ö†Ô∏è  Could not read image: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        faces = app.get(img)\n",
    "        if len(faces) == 0:\n",
    "            print(f\"‚ö†Ô∏è  No face detected in {img_file}\")\n",
    "            continue\n",
    "\n",
    "        # Take the first detected face (if multiple)\n",
    "        embedding = faces[0].embedding\n",
    "        embeddings_list.append(embedding)\n",
    "\n",
    "    if len(embeddings_list) > 0:\n",
    "        face_database[person] = np.stack(embeddings_list)\n",
    "        print(f\"‚úÖ {person}: {len(embeddings_list)} embeddings\")\n",
    "\n",
    "# Save embeddings to file\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(face_database, f)\n",
    "\n",
    "print(f\"üéâ Face embeddings saved to: {output_file}\")\n",
    "print(f\"Total persons: {len(face_database)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c1f822d-15fb-451d-bd1c-d9968e5fe15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Laurentti', 'Steve'])\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"face_database.pkl\", \"rb\") as f:\n",
    "    db = pickle.load(f)\n",
    "\n",
    "print(db.keys())       # Should show your person(s)\n",
    "print(len(db[\"Laurentti\"]))  # Should show 50 embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6760b054-89c0-4eb2-b79a-7e0b0121b98e",
   "metadata": {},
   "source": [
    "### Train InsightFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840547cc-e4fa-46f7-8c14-57584fc5cb4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (320, 320)\n",
      "üöÄ Processing Laurentti...\n",
      "‚úÖ Laurentti: 50 embeddings\n",
      "üöÄ Processing Steve...\n",
      "‚úÖ Steve: 50 embeddings\n",
      "üéâ Face database saved to: /home/steve/Python/Emerging-Technologies-in-CpE/real_time_face_recognition/face_database.pkl\n",
      "Total persons: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "DATASET_FOLDER = \"dataset-standardized\"  # Each subfolder = person\n",
    "OUTPUT_FILE = \"face_database.pkl\"\n",
    "DET_SIZE = (320, 320)  # Detection size (smaller -> more sensitive to small faces)\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize InsightFace\n",
    "# -----------------------------\n",
    "app = FaceAnalysis(providers=['CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=DET_SIZE)\n",
    "\n",
    "# -----------------------------\n",
    "# Build face database\n",
    "# -----------------------------\n",
    "face_database = {}\n",
    "\n",
    "for person_name in sorted(os.listdir(DATASET_FOLDER)):\n",
    "    person_path = os.path.join(DATASET_FOLDER, person_name)\n",
    "    if not os.path.isdir(person_path):\n",
    "        continue\n",
    "\n",
    "    embeddings = []\n",
    "    print(f\"üöÄ Processing {person_name}...\")\n",
    "\n",
    "    for img_file in sorted(os.listdir(person_path)):\n",
    "        if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            continue\n",
    "        \n",
    "        img_path = os.path.join(person_path, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"‚ö†Ô∏è  Could not read {img_file}\")\n",
    "            continue\n",
    "\n",
    "        faces = app.get(img)\n",
    "        if not faces:\n",
    "            print(f\"‚ö†Ô∏è  No face detected in {img_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Take the first detected face embedding\n",
    "        embeddings.append(faces[0].embedding)\n",
    "\n",
    "    if embeddings:\n",
    "        face_database[person_name] = np.array(embeddings)\n",
    "        print(f\"‚úÖ {person_name}: {len(embeddings)} embeddings\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  No valid embeddings for {person_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Save the database\n",
    "# -----------------------------\n",
    "with open(OUTPUT_FILE, \"wb\") as f:\n",
    "    pickle.dump(face_database, f)\n",
    "\n",
    "print(f\"üéâ Face database saved to: {os.path.abspath(OUTPUT_FILE)}\")\n",
    "print(f\"Total persons: {len(face_database)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe4e48-c0b5-4bb2-bcf1-48b26a21b7b1",
   "metadata": {},
   "source": [
    "### Run the Facial Recognition Display (For Linux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2abf7875-d69e-4d9d-a167-18a2692bba65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['/home/steve/venvs/facerec/bin/python', 'det...>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "env = os.environ.copy()\n",
    "env['QT_QPA_PLATFORM'] = 'xcb'\n",
    "subprocess.Popen([\"/home/steve/venvs/facerec/bin/python\", \"detect.py\"], env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a729d714-123f-4d9e-acf5-36868ea934c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run the Facial Recognition Display (For Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3fae14-a78a-4604-9b91-8972269580b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!start cmd /k \"python interactive.py\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facial-recognition",
   "language": "python",
   "name": "facerec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
