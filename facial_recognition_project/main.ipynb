{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1058beab-dc9d-42b9-bbbc-72f617f6b690",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Folder Structure Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f11899-2d43-416a-89bf-5a6b367d0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic structure\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def create_initial_structure():\n",
    "    base_dir = Path(\"/home/steve/Python/Emerging-Technologies-in-CpE/facial_recognition_project\").expanduser()\n",
    "    \n",
    "    directories = [\n",
    "        'dataset_raw',           # Your original images\n",
    "        'dataset_processed/original_annotated',  # Standardized images + annotations\n",
    "        'dataset_processed/augmented',           # Augmented images with auto annotations\n",
    "        'dataset_processed/train',\n",
    "        'dataset_processed/val', \n",
    "        'models',\n",
    "        'annotations'            # LabelImg XML files\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        (base_dir / directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return base_dir\n",
    "\n",
    "base_dir = create_initial_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef1980-8590-46c0-996d-484ca6e9d8b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Standardize Original Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1b30bf2-58e1-496c-b04a-8752fd32fb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preparing Images for Annotation ===\n",
      "Standardizing Kristina: 5 images\n",
      "  Prepared: Kristina_02.jpg -> Kristina_01.jpg\n",
      "  Prepared: Kristina_01.jpg -> Kristina_02.jpg\n",
      "  Prepared: Kristina_03.jpg -> Kristina_03.jpg\n",
      "  Prepared: Kristina_04.jpg -> Kristina_04.jpg\n",
      "  Prepared: Kristina_05.jpg -> Kristina_05.jpg\n",
      "Standardizing Tonyboy: 5 images\n",
      "  Prepared: Tonyboy_02.jpg -> Tonyboy_01.jpg\n",
      "  Prepared: Tonyboy_03.jpg -> Tonyboy_02.jpg\n",
      "  Prepared: Tonyboy_01.jpg -> Tonyboy_03.jpg\n",
      "  Prepared: Tonyboy_04.jpg -> Tonyboy_04.jpg\n",
      "  Prepared: Tonyboy_05.jpg -> Tonyboy_05.jpg\n",
      "Standardizing Dave: 5 images\n",
      "  Prepared: Dave_05.jpg -> Dave_01.jpg\n",
      "  Prepared: Dave_02.jpg -> Dave_02.jpg\n",
      "  Prepared: Dave_04.jpg -> Dave_03.jpg\n",
      "  Prepared: Dave_03.jpg -> Dave_04.jpg\n",
      "  Prepared: Dave_01.jpg -> Dave_05.jpg\n",
      "Standardizing Cyril: 5 images\n",
      "  Prepared: Cyril_05.jpg -> Cyril_01.jpg\n",
      "  Prepared: Cyril_02.jpg -> Cyril_02.jpg\n",
      "  Prepared: Cyril_03.jpg -> Cyril_03.jpg\n",
      "  Prepared: Cyril_04.jpg -> Cyril_04.jpg\n",
      "  Prepared: Cyril_01.jpg -> Cyril_05.jpg\n",
      "Standardizing .comments: 0 images\n",
      "Standardizing Mars: 5 images\n",
      "  Prepared: Mars_05.jpg -> Mars_01.jpg\n",
      "  Prepared: Mars_04.jpg -> Mars_02.jpg\n",
      "  Prepared: Mars_02.jpg -> Mars_03.jpg\n",
      "  Prepared: Mars_03.jpg -> Mars_04.jpg\n",
      "  Prepared: Mars_01.jpg -> Mars_05.jpg\n",
      "Standardizing Steve: 5 images\n",
      "  Prepared: Steve_02.jpg -> Steve_01.jpg\n",
      "  Prepared: Steve_03.jpg -> Steve_02.jpg\n",
      "  Prepared: Steve_05.jpg -> Steve_03.jpg\n",
      "  Prepared: Steve_01.jpg -> Steve_04.jpg\n",
      "  Prepared: Steve_04.jpg -> Steve_05.jpg\n",
      "Standardizing Lovely: 5 images\n",
      "  Prepared: Lovely_02.jpg -> Lovely_01.jpg\n",
      "  Prepared: Lovely_04.jpg -> Lovely_02.jpg\n",
      "  Prepared: Lovely_05.jpg -> Lovely_03.jpg\n",
      "  Prepared: Lovely_03.jpg -> Lovely_04.jpg\n",
      "  Prepared: Lovely_01.jpg -> Lovely_05.jpg\n",
      "Standardizing Sheryl: 5 images\n",
      "  Prepared: Sheryl_05.jpg -> Sheryl_01.jpg\n",
      "  Prepared: Sheryl_04.jpg -> Sheryl_02.jpg\n",
      "  Prepared: Sheryl_02.jpg -> Sheryl_03.jpg\n",
      "  Prepared: Sheryl_03.jpg -> Sheryl_04.jpg\n",
      "  Prepared: Sheryl_01.jpg -> Sheryl_05.jpg\n",
      "Standardizing Laurentti: 5 images\n",
      "  Prepared: Laurentti_02.jpg -> Laurentti_01.jpg\n",
      "  Prepared: Laurentti_03.jpg -> Laurentti_02.jpg\n",
      "  Prepared: Laurentti_05.jpg -> Laurentti_03.jpg\n",
      "  Prepared: Laurentti_04.jpg -> Laurentti_04.jpg\n",
      "  Prepared: Laurentti_01.jpg -> Laurentti_05.jpg\n",
      "Standardizing Danica: 5 images\n",
      "  Prepared: Danica_04.jpg -> Danica_01.jpg\n",
      "  Prepared: Danica_02.jpg -> Danica_02.jpg\n",
      "  Prepared: Danica_01.jpg -> Danica_03.jpg\n",
      "  Prepared: Danica_05.jpg -> Danica_04.jpg\n",
      "  Prepared: Danica_03.jpg -> Danica_05.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "class DatasetPreparer:\n",
    "    def __init__(self, raw_path, output_path):\n",
    "        self.raw_path = Path(raw_path)\n",
    "        self.output_path = Path(output_path)\n",
    "    \n",
    "    def standardize_for_annotation(self):\n",
    "        \"\"\"Prepare standardized images for LabelImg annotation\"\"\"\n",
    "        print(\"=== Preparing Images for Annotation ===\")\n",
    "        \n",
    "        person_folders = [f for f in self.raw_path.iterdir() if f.is_dir()]\n",
    "        \n",
    "        for person_folder in person_folders:\n",
    "            person_name = person_folder.name\n",
    "            output_person_dir = self.output_path / 'original_annotated' / person_name\n",
    "            output_person_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            image_files = list(person_folder.glob('*.*'))\n",
    "            valid_images = [f for f in image_files if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "            \n",
    "            print(f\"Standardizing {person_name}: {len(valid_images)} images\")\n",
    "            \n",
    "            for i, img_path in enumerate(valid_images, 1):\n",
    "                try:\n",
    "                    # Read and resize image\n",
    "                    image = cv2.imread(str(img_path))\n",
    "                    if image is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Standardize size\n",
    "                    resized = cv2.resize(image, (640, 640))\n",
    "                    \n",
    "                    # Save with standardized name\n",
    "                    new_filename = f\"{person_name}_{i:02d}.jpg\"\n",
    "                    output_path = output_person_dir / new_filename\n",
    "                    cv2.imwrite(str(output_path), resized)\n",
    "                    \n",
    "                    print(f\"  Prepared: {img_path.name} -> {new_filename}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error: {img_path.name} -> {e}\")\n",
    "\n",
    "# Run standardization\n",
    "preparer = DatasetPreparer(\n",
    "    base_dir / \"dataset_raw\",\n",
    "    base_dir / \"dataset_processed\"\n",
    ")\n",
    "preparer.standardize_for_annotation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b034d-00e1-4b32-8432-210c264a6e4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Partial Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b4819ba-f64a-40f6-90bb-59b773bcf530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preparing Dave_03.jpg for Dave ===\n",
      "Success: Dave_03.jpg -> Dave_Dave_03.jpg\n",
      "=== Preparing Tonyboy_01.jpg for Tonyboy ===\n",
      "Success: Tonyboy_01.jpg -> Tonyboy_Tonyboy_01.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "class DatasetPreparer:\n",
    "    def __init__(self, raw_path, output_path):\n",
    "        self.raw_path = Path(raw_path)\n",
    "        self.output_path = Path(output_path)\n",
    "    \n",
    "    def standardize_single_image(self, image_name, person_name):\n",
    "        \"\"\"Standardize a single image and maintain folder structure\"\"\"\n",
    "        print(f\"=== Preparing {image_name} for {person_name} ===\")\n",
    "        \n",
    "        # Input path\n",
    "        input_path = self.raw_path / person_name / image_name\n",
    "        \n",
    "        if not input_path.exists():\n",
    "            print(f\"Error: Image not found at {input_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Output path\n",
    "        output_person_dir = self.output_path / 'augmented' / person_name\n",
    "        output_person_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # Read and resize image\n",
    "            image = cv2.imread(str(input_path))\n",
    "            if image is None:\n",
    "                print(f\"Error: Could not read image {image_name}\")\n",
    "                return False\n",
    "            \n",
    "            # Standardize size\n",
    "            resized = cv2.resize(image, (640, 640))\n",
    "            \n",
    "            # Save with standardized name\n",
    "            original_stem = Path(image_name).stem\n",
    "            new_filename = f\"{person_name}_{original_stem}.jpg\"\n",
    "            output_path = output_person_dir / new_filename\n",
    "            cv2.imwrite(str(output_path), resized)\n",
    "            \n",
    "            print(f\"Success: {image_name} -> {new_filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "# Simple usage for your specific case:\n",
    "preparer = DatasetPreparer(\n",
    "    base_dir / \"dataset_processed/augmented\",\n",
    "    base_dir / \"dataset_processed/augemented\"\n",
    ")\n",
    "\n",
    "\n",
    "# Process multiple specific images\n",
    "images_to_process = [\n",
    "    (\"Dave_03.jpg\", \"Dave\"),\n",
    "    (\"Tonyboy_01.jpg\", \"Tonyboy\")\n",
    "]\n",
    "\n",
    "for image_name, person_name in images_to_process:\n",
    "    preparer.standardize_single_image(image_name, person_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1982c5dc-7505-477f-ba5d-13904b861439",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Manual Annotation with LabelImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90bd772c-afdf-4096-8e1a-41e1cdd361e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ NOW RUN THESE COMMANDS IN TERMINAL:\n",
      "\n",
      "1. Activate your environment (if using conda/venv)\n",
      "2. Run: labelImg\n",
      "\n",
      "3. In LabelImg:\n",
      "   - Open Dir: ~/Py/Emerging-Technologies-in-CpE/f/dataset_processed/original_annotated/\n",
      "   - Save Dir: ~/Py/Emerging-Technologies-in-CpE/f/annotations/\n",
      "   - Format: PascalVOC (XML)\n",
      "   - Annotate ALL faces in all images\n",
      "\n",
      "4. Press Enter here when annotation is complete...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# After running the standardization above, manually run LabelImg:\n",
    "print(\"\"\"\n",
    "ðŸš€ NOW RUN THESE COMMANDS IN TERMINAL:\n",
    "\n",
    "1. Activate your environment (if using conda/venv)\n",
    "2. Run: labelImg\n",
    "\n",
    "3. In LabelImg:\n",
    "   - Open Dir: ~/Py/Emerging-Technologies-in-CpE/f/dataset_processed/original_annotated/\n",
    "   - Save Dir: ~/Py/Emerging-Technologies-in-CpE/f/annotations/\n",
    "   - Format: PascalVOC (XML)\n",
    "   - Annotate ALL faces in all images\n",
    "\n",
    "4. Press Enter here when annotation is complete...\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ff57d-906a-4099-96d5-a3331abc60ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dataset Augementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7514e98-c90a-4e87-b49c-8cee104f693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Installing/Checking Requirements ===\n",
      "ðŸ“¦ Installing opencv-python...\n",
      "Requirement already satisfied: opencv-python in /home/steve/venvs/facerec/lib/python3.13/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /home/steve/venvs/facerec/lib/python3.13/site-packages (from opencv-python) (2.2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/home/steve/venvs/facerec/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… numpy already installed\n",
      "ðŸ“¦ Installing Pillow...\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3.13/site-packages (11.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/home/steve/venvs/facerec/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… insightface already installed\n",
      "ðŸ“¦ Installing scikit-learn...\n",
      "Requirement already satisfied: scikit-learn in /home/steve/venvs/facerec/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/steve/venvs/facerec/lib/python3.13/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/steve/venvs/facerec/lib/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/steve/venvs/facerec/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/steve/venvs/facerec/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/home/steve/venvs/facerec/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Augmentation with Simple Augmentations ===\n",
      "=== Augmenting Images with Annotation Propagation ===\n",
      "Loaded annotations for 50 images\n",
      "Augmenting Kristina: 5 images\n",
      "  Completed Kristina\n",
      "Augmenting Tonyboy: 5 images\n",
      "  Completed Tonyboy\n",
      "Augmenting Dave: 5 images\n",
      "  Completed Dave\n",
      "Augmenting Cyril: 5 images\n",
      "  Completed Cyril\n",
      "Augmenting .comments: 0 images\n",
      "  Completed .comments\n",
      "Augmenting Mars: 5 images\n",
      "  Completed Mars\n",
      "Augmenting Steve: 5 images\n",
      "  Completed Steve\n",
      "Augmenting Lovely: 5 images\n",
      "  Completed Lovely\n",
      "Augmenting Sheryl: 5 images\n",
      "  Completed Sheryl\n",
      "Augmenting Laurentti: 5 images\n",
      "  Completed Laurentti\n",
      "Augmenting Danica: 5 images\n",
      "  Completed Danica\n",
      "\n",
      "âœ… Augmentation completed!\n",
      "Original images: 50\n",
      "Total images after augmentation: 550\n",
      "Multiplication factor: 11.0x\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "from pathlib import Path\n",
    "import random\n",
    "import math\n",
    "\n",
    "class SimpleAugmentation:\n",
    "    def __init__(self):\n",
    "        self.augmentation_count = 10\n",
    "    \n",
    "    def horizontal_flip(self, image, bboxes):\n",
    "        \"\"\"Horizontal flip augmentation\"\"\"\n",
    "        flipped_image = cv2.flip(image, 1)\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        flipped_bboxes = []\n",
    "        for bbox in bboxes:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            flipped_bbox = [w - x2, y1, w - x1, y2]\n",
    "            flipped_bboxes.append(flipped_bbox)\n",
    "        \n",
    "        return flipped_image, flipped_bboxes\n",
    "    \n",
    "    def random_rotation(self, image, bboxes, angle_range=(-15, 15)):\n",
    "        \"\"\"Random rotation augmentation\"\"\"\n",
    "        angle = random.uniform(angle_range[0], angle_range[1])\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Rotation matrix\n",
    "        center = (w // 2, h // 2)\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        \n",
    "        # Rotate image\n",
    "        rotated_image = cv2.warpAffine(image, rotation_matrix, (w, h), flags=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Rotate bounding boxes\n",
    "        rotated_bboxes = []\n",
    "        for bbox in bboxes:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            \n",
    "            # Transform all four corners\n",
    "            corners = np.array([\n",
    "                [x1, y1, 1],\n",
    "                [x2, y1, 1],\n",
    "                [x2, y2, 1],\n",
    "                [x1, y2, 1]\n",
    "            ])\n",
    "            \n",
    "            transformed_corners = np.dot(corners, rotation_matrix.T)\n",
    "            new_x1 = int(transformed_corners[:, 0].min())\n",
    "            new_y1 = int(transformed_corners[:, 1].min())\n",
    "            new_x2 = int(transformed_corners[:, 0].max())\n",
    "            new_y2 = int(transformed_corners[:, 1].max())\n",
    "            \n",
    "            # Ensure within image bounds\n",
    "            new_x1 = max(0, new_x1)\n",
    "            new_y1 = max(0, new_y1)\n",
    "            new_x2 = min(w, new_x2)\n",
    "            new_y2 = min(h, new_y2)\n",
    "            \n",
    "            if new_x2 > new_x1 and new_y2 > new_y1:  # Valid bbox\n",
    "                rotated_bboxes.append([new_x1, new_y1, new_x2, new_y2])\n",
    "        \n",
    "        return rotated_image, rotated_bboxes\n",
    "    \n",
    "    def random_brightness_contrast(self, image, bboxes):\n",
    "        \"\"\"Random brightness and contrast adjustment\"\"\"\n",
    "        # Brightness adjustment\n",
    "        brightness = random.uniform(0.7, 1.3)\n",
    "        # Contrast adjustment\n",
    "        contrast = random.uniform(0.7, 1.3)\n",
    "        \n",
    "        # Apply brightness and contrast\n",
    "        adjusted_image = cv2.convertScaleAbs(image, alpha=contrast, beta=(brightness - 1) * 128)\n",
    "        \n",
    "        return adjusted_image, bboxes  # Bboxes remain the same\n",
    "    \n",
    "    def random_scale(self, image, bboxes, scale_range=(0.8, 1.2)):\n",
    "        \"\"\"Random scaling augmentation\"\"\"\n",
    "        scale = random.uniform(scale_range[0], scale_range[1])\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        \n",
    "        # Resize image\n",
    "        scaled_image = cv2.resize(image, (new_w, new_h))\n",
    "        \n",
    "        # Pad or crop to original size\n",
    "        if scale < 1.0:\n",
    "            # Pad to original size\n",
    "            pad_x = (w - new_w) // 2\n",
    "            pad_y = (h - new_h) // 2\n",
    "            padded_image = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "            padded_image[pad_y:pad_y+new_h, pad_x:pad_x+new_w] = scaled_image\n",
    "            final_image = padded_image\n",
    "            \n",
    "            # Adjust bboxes\n",
    "            scaled_bboxes = []\n",
    "            for bbox in bboxes:\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                new_x1 = int(x1 * scale) + pad_x\n",
    "                new_y1 = int(y1 * scale) + pad_y\n",
    "                new_x2 = int(x2 * scale) + pad_x\n",
    "                new_y2 = int(y2 * scale) + pad_y\n",
    "                scaled_bboxes.append([new_x1, new_y1, new_x2, new_y2])\n",
    "                \n",
    "        else:\n",
    "            # Crop to original size\n",
    "            start_x = (new_w - w) // 2\n",
    "            start_y = (new_h - h) // 2\n",
    "            final_image = scaled_image[start_y:start_y+h, start_x:start_x+w]\n",
    "            \n",
    "            # Adjust bboxes\n",
    "            scaled_bboxes = []\n",
    "            for bbox in bboxes:\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                new_x1 = max(0, int(x1 * scale) - start_x)\n",
    "                new_y1 = max(0, int(y1 * scale) - start_y)\n",
    "                new_x2 = min(w, int(x2 * scale) - start_x)\n",
    "                new_y2 = min(h, int(y2 * scale) - start_y)\n",
    "                if new_x2 > new_x1 and new_y2 > new_y1:  # Valid bbox\n",
    "                    scaled_bboxes.append([new_x1, new_y1, new_x2, new_y2])\n",
    "        \n",
    "        return final_image, scaled_bboxes\n",
    "    \n",
    "    def add_gaussian_noise(self, image, bboxes):\n",
    "        \"\"\"Add Gaussian noise to image\"\"\"\n",
    "        noise = np.random.normal(0, 25, image.shape).astype(np.uint8)\n",
    "        noisy_image = cv2.add(image, noise)\n",
    "        return noisy_image, bboxes\n",
    "    \n",
    "    def apply_augmentation(self, image, bboxes, aug_type):\n",
    "        \"\"\"Apply specific augmentation type\"\"\"\n",
    "        if aug_type == 'flip':\n",
    "            return self.horizontal_flip(image, bboxes)\n",
    "        elif aug_type == 'rotation':\n",
    "            return self.random_rotation(image, bboxes)\n",
    "        elif aug_type == 'brightness':\n",
    "            return self.random_brightness_contrast(image, bboxes)\n",
    "        elif aug_type == 'scale':\n",
    "            return self.random_scale(image, bboxes)\n",
    "        elif aug_type == 'noise':\n",
    "            return self.add_gaussian_noise(image, bboxes)\n",
    "        else:\n",
    "            return image, bboxes\n",
    "\n",
    "class AugmentationWithAnnotations:\n",
    "    def __init__(self, annotated_images_path, annotations_path, output_path):\n",
    "        self.annotated_images_path = Path(annotated_images_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.output_path = Path(output_path)\n",
    "        self.augmentation_count = 10\n",
    "        self.augmentor = SimpleAugmentation()\n",
    "        \n",
    "    def load_annotations(self):\n",
    "        \"\"\"Load all XML annotations into a dictionary\"\"\"\n",
    "        annotations = {}\n",
    "        \n",
    "        for xml_file in self.annotations_path.glob('*.xml'):\n",
    "            try:\n",
    "                tree = ET.parse(xml_file)\n",
    "                root = tree.getroot()\n",
    "                \n",
    "                filename = root.find('filename').text\n",
    "                folder_elem = root.find('folder')\n",
    "                folder = folder_elem.text if folder_elem is not None else \"\"\n",
    "                \n",
    "                # Store bounding boxes\n",
    "                bboxes = []\n",
    "                for obj in root.findall('object'):\n",
    "                    label = obj.find('name').text\n",
    "                    bndbox = obj.find('bndbox')\n",
    "                    xmin = int(float(bndbox.find('xmin').text))\n",
    "                    ymin = int(float(bndbox.find('ymin').text))\n",
    "                    xmax = int(float(bndbox.find('xmax').text))\n",
    "                    ymax = int(float(bndbox.find('ymax').text))\n",
    "                    \n",
    "                    bboxes.append({\n",
    "                        'label': label,\n",
    "                        'bbox': [xmin, ymin, xmax, ymax]\n",
    "                    })\n",
    "                \n",
    "                # Use filename as key\n",
    "                annotations[filename] = bboxes\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {xml_file}: {e}\")\n",
    "        \n",
    "        print(f\"Loaded annotations for {len(annotations)} images\")\n",
    "        return annotations\n",
    "    \n",
    "    def augment_with_annotations(self):\n",
    "        \"\"\"Apply augmentation and propagate annotations\"\"\"\n",
    "        print(\"=== Augmenting Images with Annotation Propagation ===\")\n",
    "        \n",
    "        annotations = self.load_annotations()\n",
    "        \n",
    "        # Define augmentation types\n",
    "        augmentation_types = ['flip', 'rotation', 'brightness', 'scale', 'noise']\n",
    "        \n",
    "        person_folders = [f for f in self.annotated_images_path.iterdir() if f.is_dir()]\n",
    "        \n",
    "        total_original = 0\n",
    "        total_augmented = 0\n",
    "        \n",
    "        for person_folder in person_folders:\n",
    "            person_name = person_folder.name\n",
    "            output_person_dir = self.output_path / 'augmented' / person_name\n",
    "            output_person_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            image_files = list(person_folder.glob('*.jpg'))\n",
    "            total_original += len(image_files)\n",
    "            \n",
    "            print(f\"Augmenting {person_name}: {len(image_files)} images\")\n",
    "            \n",
    "            for img_path in image_files:\n",
    "                # Load image\n",
    "                image = cv2.imread(str(img_path))\n",
    "                if image is None:\n",
    "                    print(f\"  Could not read: {img_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Get annotations for this image\n",
    "                if img_path.name not in annotations:\n",
    "                    print(f\"  No annotations found for {img_path.name}\")\n",
    "                    continue\n",
    "                \n",
    "                bboxes_data = annotations[img_path.name]\n",
    "                bboxes = [data['bbox'] for data in bboxes_data]\n",
    "                labels = [data['label'] for data in bboxes_data]\n",
    "                \n",
    "                # Save original with annotations\n",
    "                original_output_path = output_person_dir / f\"{img_path.stem}_original.jpg\"\n",
    "                cv2.imwrite(str(original_output_path), image)\n",
    "                self.save_annotation(original_output_path, bboxes_data, person_name)\n",
    "                total_augmented += 1\n",
    "                \n",
    "                # Create augmented versions using different augmentation types\n",
    "                aug_types_used = random.sample(augmentation_types * 2, self.augmentation_count)\n",
    "                \n",
    "                for aug_idx, aug_type in enumerate(aug_types_used):\n",
    "                    try:\n",
    "                        # Apply augmentation\n",
    "                        augmented_image, augmented_bboxes = self.augmentor.apply_augmentation(\n",
    "                            image.copy(), bboxes, aug_type\n",
    "                        )\n",
    "                        \n",
    "                        # Prepare augmented annotation data\n",
    "                        aug_annotation_data = []\n",
    "                        for bbox, label in zip(augmented_bboxes, labels):\n",
    "                            # Ensure bbox coordinates are within image bounds\n",
    "                            xmin = max(0, int(bbox[0]))\n",
    "                            ymin = max(0, int(bbox[1]))\n",
    "                            xmax = min(640, int(bbox[2]))  # 640 is our standardized size\n",
    "                            ymax = min(640, int(bbox[3]))\n",
    "                            \n",
    "                            if xmin < xmax and ymin < ymax:  # Valid bbox\n",
    "                                aug_annotation_data.append({\n",
    "                                    'label': label,\n",
    "                                    'bbox': [xmin, ymin, xmax, ymax]\n",
    "                                })\n",
    "                        \n",
    "                        # Only save if we have valid annotations\n",
    "                        if aug_annotation_data:\n",
    "                            # Save augmented image\n",
    "                            aug_output_path = output_person_dir / f\"{img_path.stem}_{aug_type}_{aug_idx+1:02d}.jpg\"\n",
    "                            cv2.imwrite(str(aug_output_path), augmented_image)\n",
    "                            \n",
    "                            # Save augmented annotation\n",
    "                            self.save_annotation(aug_output_path, aug_annotation_data, person_name)\n",
    "                            total_augmented += 1\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error augmenting {img_path} with {aug_type}: {e}\")\n",
    "            \n",
    "            print(f\"  Completed {person_name}\")\n",
    "        \n",
    "        print(f\"\\nâœ… Augmentation completed!\")\n",
    "        print(f\"Original images: {total_original}\")\n",
    "        print(f\"Total images after augmentation: {total_augmented}\")\n",
    "        print(f\"Multiplication factor: {total_augmented/total_original:.1f}x\")\n",
    "    \n",
    "    def save_annotation(self, image_path, bboxes_data, folder_name):\n",
    "        \"\"\"Save annotation as XML file\"\"\"\n",
    "        image_path = Path(image_path)\n",
    "        annotation_path = self.annotations_path / f\"{image_path.stem}.xml\"\n",
    "        \n",
    "        # Create XML structure\n",
    "        annotation = ET.Element('annotation')\n",
    "        \n",
    "        # Add folder and filename\n",
    "        folder_elem = ET.SubElement(annotation, 'folder')\n",
    "        folder_elem.text = folder_name\n",
    "        \n",
    "        filename_elem = ET.SubElement(annotation, 'filename')\n",
    "        filename_elem.text = image_path.name\n",
    "        \n",
    "        # Add size (assuming 640x640 from standardization)\n",
    "        size_elem = ET.SubElement(annotation, 'size')\n",
    "        ET.SubElement(size_elem, 'width').text = '640'\n",
    "        ET.SubElement(size_elem, 'height').text = '640'\n",
    "        ET.SubElement(size_elem, 'depth').text = '3'\n",
    "        \n",
    "        # Add each object/bbox\n",
    "        for bbox_data in bboxes_data:\n",
    "            obj_elem = ET.SubElement(annotation, 'object')\n",
    "            ET.SubElement(obj_elem, 'name').text = bbox_data['label']\n",
    "            ET.SubElement(obj_elem, 'pose').text = 'Unspecified'\n",
    "            ET.SubElement(obj_elem, 'truncated').text = '0'\n",
    "            ET.SubElement(obj_elem, 'difficult').text = '0'\n",
    "            \n",
    "            bndbox_elem = ET.SubElement(obj_elem, 'bndbox')\n",
    "            xmin, ymin, xmax, ymax = bbox_data['bbox']\n",
    "            ET.SubElement(bndbox_elem, 'xmin').text = str(xmin)\n",
    "            ET.SubElement(bndbox_elem, 'ymin').text = str(ymin)\n",
    "            ET.SubElement(bndbox_elem, 'xmax').text = str(xmax)\n",
    "            ET.SubElement(bndbox_elem, 'ymax').text = str(ymax)\n",
    "        \n",
    "        # Save XML file\n",
    "        xml_str = minidom.parseString(ET.tostring(annotation)).toprettyxml(indent=\"  \")\n",
    "        with open(annotation_path, 'w') as f:\n",
    "            f.write(xml_str)\n",
    "\n",
    "# Install only what we need\n",
    "def install_requirements():\n",
    "    \"\"\"Install required packages without PyTorch dependencies\"\"\"\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    packages = [\n",
    "        \"opencv-python\",\n",
    "        \"numpy\", \n",
    "        \"Pillow\",\n",
    "        \"insightface\",\n",
    "        \"scikit-learn\"\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.replace(\"-\", \"_\"))\n",
    "            print(f\"âœ… {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"ðŸ“¦ Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install requirements\n",
    "print(\"=== Installing/Checking Requirements ===\")\n",
    "install_requirements()\n",
    "\n",
    "# Now run the augmentation\n",
    "print(\"\\n=== Starting Augmentation with Simple Augmentations ===\")\n",
    "\n",
    "# Define paths\n",
    "base_dir = Path(\"/home/steve/Python/Emerging-Technologies-in-CpE/facial_recognition_project\").expanduser()\n",
    "\n",
    "# Run augmentation with annotation propagation\n",
    "augmenter = AugmentationWithAnnotations(\n",
    "    base_dir / \"dataset_processed\" / \"original_annotated\",\n",
    "    base_dir / \"annotations\", \n",
    "    base_dir / \"dataset_processed\"\n",
    ")\n",
    "augmenter.augment_with_annotations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c9675-0b88-4ee8-97b1-650636005128",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Convert all annotations to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1f042c4-9083-4909-854b-e68e065cb13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 5: Converting Annotations to JSON ===\n",
      "Found 50 annotation files to convert\n",
      "âœ… Converted 50 annotations to /home/steve/Python/Emerging-Technologies-in-CpE/facial_recognition_project/dataset_processed/annotations.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "class AnnotationConverter:\n",
    "    def __init__(self, annotations_path, output_path):\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.output_path = Path(output_path)\n",
    "        \n",
    "    def convert_all_annotations_to_json(self):\n",
    "        \"\"\"Convert all XML annotations to a single JSON file for InsightFace\"\"\"\n",
    "        all_annotations = {}\n",
    "        \n",
    "        # Get all XML files (original + augmented)\n",
    "        xml_files = list(self.annotations_path.glob('*.xml'))\n",
    "        print(f\"Found {len(xml_files)} annotation files to convert\")\n",
    "        \n",
    "        for xml_file in xml_files:\n",
    "            try:\n",
    "                tree = ET.parse(xml_file)\n",
    "                root = tree.getroot()\n",
    "                \n",
    "                filename = root.find('filename').text\n",
    "                folder_elem = root.find('folder')\n",
    "                folder = folder_elem.text if folder_elem is not None else \"\"\n",
    "                \n",
    "                # Construct image path\n",
    "                if folder:\n",
    "                    image_path = f\"dataset_processed/augmented/{folder}/{filename}\"\n",
    "                else:\n",
    "                    # Try to find which folder this image belongs to\n",
    "                    image_path = self.find_image_path(filename)\n",
    "                \n",
    "                # Extract bounding boxes\n",
    "                bboxes = []\n",
    "                for obj in root.findall('object'):\n",
    "                    label = obj.find('name').text\n",
    "                    bndbox = obj.find('bndbox')\n",
    "                    xmin = int(float(bndbox.find('xmin').text))\n",
    "                    ymin = int(float(bndbox.find('ymin').text))\n",
    "                    xmax = int(float(bndbox.find('xmax').text))\n",
    "                    ymax = int(float(bndbox.find('ymax').text))\n",
    "                    \n",
    "                    bboxes.append({\n",
    "                        'label': label,\n",
    "                        'bbox': [xmin, ymin, xmax, ymax]\n",
    "                    })\n",
    "                \n",
    "                if bboxes:  # Only add if we have annotations\n",
    "                    all_annotations[image_path] = bboxes\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {xml_file}: {e}\")\n",
    "        \n",
    "        # Save as JSON\n",
    "        output_file = self.output_path / \"annotations.json\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(all_annotations, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ… Converted {len(all_annotations)} annotations to {output_file}\")\n",
    "        return all_annotations\n",
    "    \n",
    "    def find_image_path(self, filename):\n",
    "        \"\"\"Find which folder contains this image\"\"\"\n",
    "        augmented_path = base_dir / \"dataset_processed\" / \"augmented\"\n",
    "        for person_folder in augmented_path.iterdir():\n",
    "            if person_folder.is_dir():\n",
    "                image_path = person_folder / filename\n",
    "                if image_path.exists():\n",
    "                    return f\"dataset_processed/augmented/{person_folder.name}/{filename}\"\n",
    "        return f\"dataset_processed/augmented/unknown/{filename}\"\n",
    "\n",
    "# Convert annotations to JSON\n",
    "print(\"=== STEP 5: Converting Annotations to JSON ===\")\n",
    "converter = AnnotationConverter(\n",
    "    base_dir / \"annotations\",\n",
    "    base_dir / \"dataset_processed\"\n",
    ")\n",
    "annotations_json = converter.convert_all_annotations_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f822d-15fb-451d-bd1c-d9968e5fe15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6760b054-89c0-4eb2-b79a-7e0b0121b98e",
   "metadata": {},
   "source": [
    "### Train InsightFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "840547cc-e4fa-46f7-8c14-57584fc5cb4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 6: Training InsightFace Model ===\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "âœ… Found annotations file: /home/steve/Python/Emerging-Technologies-in-CpE/facial_recognition_project/dataset_processed/annotations.json\n",
      "=== Building Face Database ===\n",
      "âœ… Added embedding for Lovely from Lovely_05.jpg\n",
      "âœ… Added embedding for Dave from Dave_01.jpg\n",
      "âœ… Added embedding for Danica from Danica_04.jpg\n",
      "âœ… Added embedding for Mars from Mars_01.jpg\n",
      "âœ… Added embedding for Danica from Danica_03.jpg\n",
      "âœ… Added embedding for Dave from Dave_02.jpg\n",
      "âœ… Added embedding for Lovely from Lovely_03.jpg\n",
      "âœ… Added embedding for Mars from Mars_04.jpg\n",
      "âœ… Added embedding for Mars from Mars_03.jpg\n",
      "âœ… Added embedding for Laurentti from Laurentti_03.jpg\n",
      "âœ… Added embedding for Tonyboy from Tonyboy_03.jpg\n",
      "âœ… Added embedding for Mars from Mars_02.jpg\n",
      "âœ… Added embedding for Sheryl from Sheryl_01.jpg\n",
      "âœ… Added embedding for Steve from Steve_01.jpg\n",
      "âœ… Added embedding for Steve from Steve_05.jpg\n",
      "âœ… Added embedding for Lovely from Lovely_04.jpg\n",
      "âœ… Added embedding for Kristina from Kristina_05.jpg\n",
      "âœ… Added embedding for Mars from Mars_05.jpg\n",
      "âœ… Added embedding for Laurentti from Laurentti_02.jpg\n",
      "âœ… Added embedding for Tonyboy from Tonyboy_04.jpg\n",
      "âœ… Added embedding for Sheryl from Sheryl_03.jpg\n",
      "âœ… Added embedding for Kristina from Kristina_03.jpg\n",
      "âœ… Added embedding for Sheryl from Sheryl_04.jpg\n",
      "âœ… Added embedding for Tonyboy from Tonyboy_01.jpg\n",
      "âœ… Added embedding for Laurentti from Laurentti_05.jpg\n",
      "âœ… Added embedding for Dave from Dave_03.jpg\n",
      "âœ… Added embedding for Dave from Dave_03.jpg\n",
      "âœ… Added embedding for Cyril from Cyril_04.jpg\n",
      "âœ… Added embedding for Cyril from Cyril_05.jpg\n",
      "âœ… Added embedding for Laurentti from Laurentti_01.jpg\n",
      "âœ… Added embedding for Steve from Steve_02.jpg\n",
      "âœ… Added embedding for Tonyboy from Tonyboy_02.jpg\n",
      "âœ… Added embedding for Tonyboy from Tonyboy_05.jpg\n",
      "âœ… Added embedding for Lovely from Lovely_01.jpg\n",
      "âœ… Added embedding for Cyril from Cyril_02.jpg\n",
      "âœ… Added embedding for Danica from Danica_01.jpg\n",
      "âœ… Added embedding for Kristina from Kristina_04.jpg\n",
      "âœ… Added embedding for Laurentti from Laurentti_04.jpg\n",
      "âœ… Added embedding for Steve from Steve_03.jpg\n",
      "âœ… Added embedding for Dave from Dave_05.jpg\n",
      "âœ… Added embedding for Cyril from Cyril_03.jpg\n",
      "âœ… Added embedding for Cyril from Cyril_01.jpg\n",
      "âœ… Added embedding for Danica from Danica_02.jpg\n",
      "âœ… Added embedding for Steve from Steve_04.jpg\n",
      "âœ… Added embedding for Sheryl from Sheryl_02.jpg\n",
      "âœ… Added embedding for Lovely from Lovely_02.jpg\n",
      "âœ… Added embedding for Danica from Danica_05.jpg\n",
      "âœ… Added embedding for Sheryl from Sheryl_05.jpg\n",
      "âœ… Added embedding for Kristina from Kristina_02.jpg\n",
      "âœ… Added embedding for Kristina from Kristina_01.jpg\n",
      "âœ… Added embedding for Dave from Dave_04.jpg\n",
      "\n",
      "Database built successfully!\n",
      "Processed: 51 faces\n",
      "Skipped: 0 faces\n",
      "Total persons: 10\n",
      "  Lovely: 5 embeddings\n",
      "  Dave: 6 embeddings\n",
      "  Danica: 5 embeddings\n",
      "  Mars: 5 embeddings\n",
      "  Laurentti: 5 embeddings\n",
      "  Tonyboy: 5 embeddings\n",
      "  Sheryl: 5 embeddings\n",
      "  Steve: 5 embeddings\n",
      "  Kristina: 5 embeddings\n",
      "  Cyril: 5 embeddings\n",
      "âœ… Face database saved to /home/steve/Python/Emerging-Technologies-in-CpE/facial_recognition_project/models/face_database.pkl\n",
      "\n",
      "ðŸŽ‰ Training completed successfully!\n",
      "   Persons in database: 10\n",
      "   Total face embeddings: 51\n"
     ]
    }
   ],
   "source": [
    "from insightface.app import FaceAnalysis\n",
    "import pickle\n",
    "\n",
    "class InsightFaceTrainer:\n",
    "    def __init__(self, model_name='buffalo_l'):\n",
    "        self.model = FaceAnalysis(name=model_name, providers=['CPUExecutionProvider'])\n",
    "        self.model.prepare(ctx_id=0, det_size=(640, 640))\n",
    "        self.face_database = {}  # {person_id: [embedding1, embedding2, ...]}\n",
    "        self.threshold = 0.65  # Similarity threshold\n",
    "    \n",
    "    def build_face_database(self, annotations_file):\n",
    "        \"\"\"Build face database from annotations - WITH FALLBACK DETECTION\"\"\"\n",
    "        print(\"=== Building Face Database ===\")\n",
    "        \n",
    "        with open(annotations_file, 'r') as f:\n",
    "            annotations = json.load(f)\n",
    "        \n",
    "        processed_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        for image_path, objects in annotations.items():\n",
    "            full_image_path = base_dir / image_path\n",
    "            \n",
    "            if not full_image_path.exists():\n",
    "                print(f\"Image not found: {full_image_path}\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            for obj in objects:\n",
    "                person_id = obj['label']\n",
    "                bbox = obj['bbox']\n",
    "                \n",
    "                # Try multiple methods to extract face embedding\n",
    "                embedding = self.extract_face_embedding_robust(str(full_image_path), bbox)\n",
    "                \n",
    "                if embedding is not None:\n",
    "                    if person_id not in self.face_database:\n",
    "                        self.face_database[person_id] = []\n",
    "                    \n",
    "                    self.face_database[person_id].append(embedding)\n",
    "                    processed_count += 1\n",
    "                    print(f\"âœ… Added embedding for {person_id} from {Path(image_path).name}\")\n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "                    print(f\"âŒ Failed to extract embedding from {Path(image_path).name}\")\n",
    "        \n",
    "        print(f\"\\nDatabase built successfully!\")\n",
    "        print(f\"Processed: {processed_count} faces\")\n",
    "        print(f\"Skipped: {skipped_count} faces\")\n",
    "        print(f\"Total persons: {len(self.face_database)}\")\n",
    "        \n",
    "        # Print person-wise counts\n",
    "        for person, embeddings in self.face_database.items():\n",
    "            print(f\"  {person}: {len(embeddings)} embeddings\")\n",
    "    \n",
    "    def extract_face_embedding_robust(self, image_path, bbox):\n",
    "        \"\"\"Robust face extraction with multiple fallback methods\"\"\"\n",
    "        try:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                return None\n",
    "            \n",
    "            # METHOD 1: Try with original bounding box\n",
    "            embedding = self._extract_with_bbox(image, bbox)\n",
    "            if embedding is not None:\n",
    "                return embedding\n",
    "            \n",
    "            # METHOD 2: Try with expanded bounding box (if bbox was too tight)\n",
    "            embedding = self._extract_with_expanded_bbox(image, bbox)\n",
    "            if embedding is not None:\n",
    "                return embedding\n",
    "            \n",
    "            # METHOD 3: Let InsightFace auto-detect face in the entire image\n",
    "            embedding = self._extract_with_auto_detect(image)\n",
    "            if embedding is not None:\n",
    "                return embedding\n",
    "            \n",
    "            return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting embedding from {Path(image_path).name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_with_bbox(self, image, bbox):\n",
    "        \"\"\"Extract using provided bounding box\"\"\"\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        \n",
    "        # Ensure bounding box is within image bounds\n",
    "        h, w = image.shape[:2]\n",
    "        x1 = max(0, x1)\n",
    "        y1 = max(0, y1)\n",
    "        x2 = min(w, x2)\n",
    "        y2 = min(h, y2)\n",
    "        \n",
    "        # Check if bbox is valid\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            return None\n",
    "        \n",
    "        # Crop face region\n",
    "        face_crop = image[y1:y2, x1:x2]\n",
    "        \n",
    "        if face_crop.size == 0:\n",
    "            return None\n",
    "        \n",
    "        # Ensure minimum size for face detection\n",
    "        if face_crop.shape[0] < 20 or face_crop.shape[1] < 20:\n",
    "            return None\n",
    "        \n",
    "        # Get face embedding from cropped region\n",
    "        faces = self.model.get(face_crop)\n",
    "        if len(faces) == 1 and hasattr(faces[0], 'embedding'):\n",
    "            return faces[0].embedding\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _extract_with_expanded_bbox(self, image, bbox, expansion_factor=0.2):\n",
    "        \"\"\"Extract using expanded bounding box\"\"\"\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Expand bounding box\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        expand_x = int(width * expansion_factor)\n",
    "        expand_y = int(height * expansion_factor)\n",
    "        \n",
    "        new_x1 = max(0, x1 - expand_x)\n",
    "        new_y1 = max(0, y1 - expand_y)\n",
    "        new_x2 = min(w, x2 + expand_x)\n",
    "        new_y2 = min(h, y2 + expand_y)\n",
    "        \n",
    "        return self._extract_with_bbox(image, [new_x1, new_y1, new_x2, new_y2])\n",
    "    \n",
    "    def _extract_with_auto_detect(self, image):\n",
    "        \"\"\"Let InsightFace auto-detect face in entire image\"\"\"\n",
    "        faces = self.model.get(image)\n",
    "        \n",
    "        # Use the largest face if multiple detected\n",
    "        if len(faces) >= 1:\n",
    "            # Find the largest face by bounding box area\n",
    "            largest_face = max(faces, key=lambda face: \n",
    "                             (face.bbox[2] - face.bbox[0]) * (face.bbox[3] - face.bbox[1]))\n",
    "            \n",
    "            if hasattr(largest_face, 'embedding'):\n",
    "                return largest_face.embedding\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def save_database(self, output_path):\n",
    "        \"\"\"Save face database to file\"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(self.face_database, f)\n",
    "        print(f\"âœ… Face database saved to {output_path}\")\n",
    "    \n",
    "    def load_database(self, input_path):\n",
    "        \"\"\"Load face database from file\"\"\"\n",
    "        with open(input_path, 'rb') as f:\n",
    "            self.face_database = pickle.load(f)\n",
    "        print(f\"âœ… Face database loaded from {input_path}\")\n",
    "\n",
    "# Train the InsightFace model\n",
    "print(\"\\n=== STEP 6: Training InsightFace Model ===\")\n",
    "trainer = InsightFaceTrainer()\n",
    "\n",
    "# Build database from annotations\n",
    "annotations_file = base_dir / \"dataset_processed\" / \"annotations.json\"\n",
    "\n",
    "# Check if annotations file exists\n",
    "if not annotations_file.exists():\n",
    "    print(f\"âŒ Annotations file not found: {annotations_file}\")\n",
    "    print(\"Please run the annotation conversion step first!\")\n",
    "else:\n",
    "    print(f\"âœ… Found annotations file: {annotations_file}\")\n",
    "    trainer.build_face_database(annotations_file)\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_path = base_dir / \"models\" / \"face_database.pkl\"\n",
    "    trainer.save_database(model_path)\n",
    "    \n",
    "    # Verify the database was created\n",
    "    if trainer.face_database:\n",
    "        print(f\"\\nðŸŽ‰ Training completed successfully!\")\n",
    "        print(f\"   Persons in database: {len(trainer.face_database)}\")\n",
    "        total_embeddings = sum(len(embeddings) for embeddings in trainer.face_database.values())\n",
    "        print(f\"   Total face embeddings: {total_embeddings}\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Training failed - no face embeddings were extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe4e48-c0b5-4bb2-bcf1-48b26a21b7b1",
   "metadata": {},
   "source": [
    "### Run the Facial Recognition Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2abf7875-d69e-4d9d-a167-18a2692bba65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['/home/steve/venvs/facerec/bin/python', 'int...>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ LAUNCHING FACE RECOGNITION SYSTEM - RESIZABLE WINDOWS\n",
      "============================================================\n",
      "ðŸ“¦ Step 1: Loading trained model...\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/steve/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "âœ… Face database loaded from /home/steve/Python/Emerging-Technologies-in-CpE/facial_recognition_project/models/face_database.pkl\n",
      "   âœ… Loaded: 10 persons, 51 embeddings\n",
      "=== Creating Enhanced Interactive Collage from ORIGINAL Images ===\n",
      "Created collage with 50 ORIGINAL images (10 rows)\n",
      "\n",
      "ðŸŽ¯ ENHANCED INTERACTIVE DISPLAY READY!\n",
      "   - Hover over ORIGINAL images to see recognition results\n",
      "   - Right window shows bounding boxes and facial landmarks\n",
      "   - âœ… WINDOWS ARE NOW RESIZABLE - drag corners to adjust size!\n",
      "   - Landmark colors: Blue=Right Eye, Cyan=Left Eye, Yellow=Nose, Red=Mouth\n",
      "   - Press 'q' to quit, 'r' for report, 's' to save examples\n",
      "   - Press 'f' to toggle fullscreen on detailed view\n",
      "\n",
      "ðŸŽ‰ Face recognition system completed!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "env = os.environ.copy()\n",
    "env['QT_QPA_PLATFORM'] = 'xcb'\n",
    "subprocess.Popen([\"/home/steve/venvs/facerec/bin/python\", \"interactive.py\"], env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1fde51-dc5f-4bbe-8bfc-c41fe07f048a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facial-recognition",
   "language": "python",
   "name": "facerec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
